{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:29:04.661355Z",
     "start_time": "2025-02-17T13:29:04.651777Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchvision.io import read_image\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "pd.options.display.max_columns = 700\n",
    "\n",
    "BASE = '/media/HDD1/yucj/OL3I/'\n",
    "ORIGINAL = join(BASE, 'original_file/')\n",
    "FEATURE = join(BASE, 'data_file_423/')\n",
    "\n",
    "from typing import List\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import k_means, SpectralClustering\n",
    "import multiprocessing as mp\n",
    "\n",
    "ANALYSIS = join(BASE, 'analysis')\n",
    "\n",
    "def conf_matrix_from_matrices(mat_gt, mat_pred):\n",
    "  overlap_and = (mat_pred & mat_gt)\n",
    "  tp = overlap_and.sum()\n",
    "  fp = mat_pred.sum()-overlap_and.sum()\n",
    "  fn = mat_gt.sum()-overlap_and.sum()\n",
    "  tn = mat_gt.shape[0]**2-(tp+fp+fn)\n",
    "  return tp, fp, fn, tn"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:27:21.697976Z",
     "start_time": "2025-02-17T13:27:21.686260Z"
    }
   },
   "source": [
    "def check_or_save(obj, path, index=None, header=None):\n",
    "  if isinstance(obj, pd.DataFrame):\n",
    "    if index is None or header is None:\n",
    "      raise ValueError('Index and header must be specified for saving a dataframe')\n",
    "    if os.path.exists(path):\n",
    "      if not header:\n",
    "        saved_df = pd.read_csv(path,header=None)\n",
    "      else:\n",
    "        saved_df = pd.read_csv(path)\n",
    "      naked_df = saved_df.reset_index(drop=True)\n",
    "      naked_df.columns = range(naked_df.shape[1])\n",
    "      naked_obj = obj.reset_index(drop=not index)\n",
    "      naked_obj.columns = range(naked_obj.shape[1])\n",
    "      if naked_df.round(6).equals(naked_obj.round(6)):\n",
    "        return\n",
    "      else:\n",
    "        diff = (naked_df.round(6) == naked_obj.round(6))\n",
    "        diff[naked_df.isnull()] = naked_df.isnull() & naked_obj.isnull()\n",
    "        assert diff.all().all(), \"Dataframe is not the same as saved dataframe\"\n",
    "    else:\n",
    "      obj.to_csv(path, index=index, header=header)\n",
    "  else:\n",
    "    if os.path.exists(path):\n",
    "      saved_obj = torch.load(path)\n",
    "      if isinstance(obj, list):\n",
    "        for i in range(len(obj)):\n",
    "          check_array_equality(obj[i], saved_obj[i])\n",
    "      else:\n",
    "        check_array_equality(obj, saved_obj)\n",
    "    else:\n",
    "      print(f'Saving to {path}')\n",
    "      torch.save(obj, path)\n",
    "\n",
    "\n",
    "def check_array_equality(ob1, ob2):\n",
    "  if torch.is_tensor(ob1) or isinstance(ob1, np.ndarray):\n",
    "    assert (ob2 == ob1).all()\n",
    "  else:\n",
    "    assert ob2 == ob1"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Tabular Dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-17T13:30:15.140960Z",
     "start_time": "2025-02-17T13:30:11.713502Z"
    }
   },
   "source": [
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "info = pd.read_csv(join(ORIGINAL, 'OL3I_tabular_info_all_423_missing_age.csv'))\n",
    "\n",
    "non_feature_columns = ['img_id', 'anon_id', 'set_1y', 'label_1y']\n",
    "feature_columns = info.columns.difference(non_feature_columns)\n",
    "print(len(feature_columns))\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler()\n",
    "info[feature_columns] = scaler.fit_transform(info[feature_columns])\n",
    "\n",
    "train_df = info.set_index('set_1y').loc['train']\n",
    "val_df = info.set_index('set_1y').loc['val']\n",
    "test_df = info.set_index('set_1y').loc['test']\n",
    "\n",
    "train_labels_all = list(train_df['label_1y'])\n",
    "val_labels_all = list(val_df['label_1y'])\n",
    "test_labels_all = list(test_df['label_1y'])\n",
    "\n",
    "lengths = [1 for i in range(len(feature_columns)-2)]\n",
    "lengths.insert(25, 2)\n",
    "# lengths.insert(15, 2)\n",
    "lengths.append(2)\n",
    "\n",
    "print(len(lengths))\n",
    "print(lengths)\n",
    "\n",
    "# check_or_save(lengths, join(FEATURE, 'OL3I_tabular_lengths.pt'))\n",
    "#\n",
    "# check_or_save(train_labels_all, join(FEATURE, 'OL3I_labels_train.pt'))\n",
    "# check_or_save(val_labels_all, join(FEATURE, 'OL3I_labels_val.pt'))\n",
    "# check_or_save(test_labels_all, join(FEATURE, 'OL3I_labels_test.pt'))\n",
    "\n",
    "check_or_save(train_df.loc[:,~train_df.columns.isin(non_feature_columns)],join(FEATURE, 'OL3I_features_train_missing_age.csv'), index=False, header=False)\n",
    "check_or_save(val_df.loc[:,~val_df.columns.isin(non_feature_columns)],join(FEATURE, 'OL3I_features_val_missing_age.csv'), index=False, header=False)\n",
    "check_or_save(test_df.loc[:,~test_df.columns.isin(non_feature_columns)],join(FEATURE, 'OL3I_features_test_missing_age.csv'), index=False, header=False)\n",
    "\n",
    "check_or_save(train_df, join(FEATURE,f'OL3I_full_features_train_missing_age.csv'), index=True, header=True)\n",
    "check_or_save(val_df, join(FEATURE,f'OL3I_full_features_val_missing_age.csv'), index=True, header=True)\n",
    "check_or_save(test_df, join(FEATURE,f'OL3I_full_features_test_missing_age.csv'), index=True, header=True)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "423\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Normalized Ims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "train 5227\n",
      "True\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/train_images.pt\n",
      "val\n",
      "val 1303\n",
      "True\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/val_images.pt\n",
      "test\n",
      "test 1609\n",
      "True\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/test_images.pt\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "h5_file = h5py.File(join(ORIGINAL, 'l3_slices.h5'), 'r')\n",
    "\n",
    "for df, t_split in zip([train_df, val_df, test_df], ['train', 'val', 'test']):\n",
    "  images = []\n",
    "  print(t_split)\n",
    "  for i,row in df.iterrows():\n",
    "    key_i = row['anon_id']\n",
    "    img_arr = np.array(h5_file[key_i])\n",
    "    img_normed = (img_arr - np.min(img_arr)) / (np.max(img_arr) - np.min(img_arr))\n",
    "    img_saved = np.expand_dims(img_normed, axis=0)\n",
    "    if np.shape(img_saved) != (1, 512, 512):\n",
    "      print(\"key_i:\", np.shape(img_saved))\n",
    "    images.append(torch.from_numpy(img_saved).float())\n",
    "  # print(img_saved)\n",
    "  print(t_split, len(images))\n",
    "  images_t = torch.stack(images).float()\n",
    "  print(images_t.is_contiguous()) \n",
    "  check_or_save(images_t, join(FEATURE, f'{t_split}_images.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Low Data Splits & Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "low-data df:  2613\n",
      "low-data labels:  2613\n",
      "low-data images:  2613\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/train_low_0.5_images.pt\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/OL3I_labels_train_low_0.5_.pt\n",
      "low-data df:  522\n",
      "low-data labels:  522\n",
      "low-data images:  522\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/train_low_0.1_images.pt\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/OL3I_labels_train_low_0.1_.pt\n",
      "low-data df:  52\n",
      "low-data labels:  52\n",
      "low-data images:  52\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/train_low_0.01_images.pt\n",
      "Saving to /media/HDD1/yucj/OL3I/data_file_423/OL3I_labels_train_low_0.01_.pt\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import h5py\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "info = pd.read_csv(join(ORIGINAL, 'OL3I_tabular_info_all_423.csv'))\n",
    "\n",
    "non_feature_columns = ['img_id', 'anon_id', 'set_1y', 'label_1y']\n",
    "feature_columns = info.columns.difference(non_feature_columns)\n",
    "print(len(feature_columns))\n",
    "\n",
    "# 归一化\n",
    "scaler = MinMaxScaler()\n",
    "info[feature_columns] = scaler.fit_transform(info[feature_columns])\n",
    "train_df = info.set_index('set_1y').loc['train']\n",
    "train_labels_all = list(train_df['label_1y'])\n",
    "\n",
    "# creat low-data training set\n",
    "for k in [0.5, 0.1, 0.01]:\n",
    "  n_samples = int(len(train_df) * k)\n",
    "  sss = StratifiedShuffleSplit(n_splits=1, test_size=n_samples, random_state=2022)\n",
    "\n",
    "  # sampling\n",
    "  for train_index, sample_index in sss.split(train_df, train_labels_all):\n",
    "      sampled_train_df = train_df.iloc[sample_index]\n",
    "      sampled_train_labels = [train_labels_all[i] for i in sample_index]\n",
    "  print('low-data df: ', len(sampled_train_df))\n",
    "  print('low-data labels: ', len(sampled_train_labels))\n",
    "\n",
    "  # images\n",
    "  h5_file = h5py.File(join(ORIGINAL, 'l3_slices.h5'), 'r')\n",
    "  images = []\n",
    "  for i,row in sampled_train_df.iterrows():\n",
    "    key_i = row['anon_id']\n",
    "    img_arr = np.array(h5_file[key_i])\n",
    "    img_normed = (img_arr - np.min(img_arr)) / (np.max(img_arr) - np.min(img_arr))\n",
    "    img_saved = np.expand_dims(img_normed, axis=0)\n",
    "    if np.shape(img_saved) != (1, 512, 512):\n",
    "      print(\"key_i:\", np.shape(img_saved))\n",
    "    images.append(torch.from_numpy(img_saved).float())\n",
    "  # print(img_saved)\n",
    "  print('low-data images: ', len(images))\n",
    "  images_t = torch.stack(images).float()\n",
    "\n",
    "  # save\n",
    "  check_or_save(images_t, join(FEATURE, f'train_low_{k}_images.pt'))\n",
    "  check_or_save(sampled_train_labels, join(FEATURE, f'OL3I_labels_train_low_{k}_.pt'))\n",
    "  check_or_save(sampled_train_df.loc[:,~train_df.columns.isin(non_feature_columns)],join(FEATURE, f'OL3I_features_train_low_{k}_.csv'), index=False, header=False)\n",
    "  check_or_save(sampled_train_df, join(FEATURE,f'OL3I_full_features_train_low_{k}_.csv'), index=True, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 'train'\n",
    "for k in [0.1, 0.01]:\n",
    "  low_data_ids = torch.load(join(FEATURES, f'{split}_ids{addendum}_{k}.pt'))\n",
    "  low_data_df = pd.read_csv(join(FEATURES,f'dvm_full_features_{split}_noOH{addendum}_{k}.csv'))\n",
    "  print(low_data_df.value_counts('Genmodel_ID'))\n",
    "  print(len(low_data_ids))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "import torch\n",
    "from os.path import join\n",
    "\n",
    "BASE = ''\n",
    "TABLES = join(BASE, 'tables_V2.0')\n",
    "FEATURES = join(BASE, 'features')\n",
    "\n",
    "train_images = torch.load(join(FEATURES, f'val_images.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=128\n",
    "\n",
    "transform = transforms.Compose([\n",
    "      transforms.RandomApply([transforms.ColorJitter(brightness=0.8, contrast=0.8, saturation=0.8)], p=0.8),\n",
    "      transforms.RandomGrayscale(p=0.2),\n",
    "      transforms.RandomApply([transforms.GaussianBlur(kernel_size=29, sigma=(0.1, 2.0))],p=0.5),\n",
    "      transforms.RandomResizedCrop(size=(img_size,img_size), scale=(0.2, 1.0), ratio=(0.75, 1.3333333333333333), antialias=True),\n",
    "      transforms.RandomHorizontalFlip(p=0.5),\n",
    "      transforms.Resize(size=(img_size,img_size), antialias=True),\n",
    "      transforms.Lambda(lambda x : x.float())\n",
    "    ])\n",
    "\n",
    "im = train_images[1]\n",
    "im_t = transform(im)\n",
    "_ = plt.imshow(im_t.permute(1,2,0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Physical Features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding missing values to physical table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill using other values\n",
    "physical_df_orig = pd.read_csv(join(FEATURES,'Ad_table (extra).csv'))\n",
    "physical_df_orig.rename(columns={' Genmodel_ID':'Genmodel_ID', ' Genmodel':'Genmodel'}, inplace=True)\n",
    "\n",
    "# Manual touches\n",
    "\n",
    "# Peugeot RCZ\n",
    "physical_df_orig.loc[physical_df_orig['Genmodel_ID'] == '69_36','Wheelbase']=2612\n",
    "# Ford Grand C-Max\n",
    "physical_df_orig.loc[physical_df_orig['Genmodel_ID'] == '29_20','Wheelbase']=2788 \n",
    "\n",
    "def fill_from_other_entry(row):\n",
    "    for attr in ['Wheelbase', 'Length', 'Width', 'Height']:\n",
    "        if pd.isna(row[attr]) or row[attr]==0:\n",
    "            other_rows = physical_df_orig.loc[physical_df_orig['Genmodel_ID']==row['Genmodel_ID']]\n",
    "            other_rows.dropna(subset=[attr], inplace=True)\n",
    "            other_rows.drop_duplicates(subset=[attr], inplace=True)\n",
    "            other_rows = other_rows[other_rows[attr]>0]\n",
    "            if len(other_rows)>0:\n",
    "                row[attr] = other_rows[attr].values[0]\n",
    "    return row\n",
    "\n",
    "physical_df_orig = physical_df_orig.apply(fill_from_other_entry, axis=1)\n",
    "\n",
    "physical_df_orig.to_csv(join(FEATURES,'Ad_table_physical_filled.csv'), index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add physical attributes to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add jitter to physical dimensions so they aren't just labels\n",
    "def add_jitter(x, jitter=50):\n",
    "    return x + random.randint(-jitter, jitter)\n",
    "\n",
    "random.seed(2022)\n",
    "physical_df = pd.read_csv(join(FEATURES,'Ad_table_physical_filled.csv'))\n",
    "for attr in ['Wheelbase', 'Length', 'Width', 'Height']:\n",
    "    physical_df[attr] = physical_df[attr].apply(add_jitter)\n",
    "physical_df.to_csv(join(FEATURES,'Ad_table_physical_filled_jittered_50.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ford ranger (29_30) has wrong height. Missing 1 in front... 805.0 instead of 1805.0\n",
    "# Mercedes Benz (59_29) wrong wheelbase, 5246.0 instead of 3106\n",
    "# Kia Rio (43_9) wrong wheelbase, 4065.0 instead of 2580\n",
    "# FIXED\n",
    "\n",
    "\n",
    "physical_df = pd.read_csv(join(FEATURES,'Ad_table_physical_filled_jittered_50.csv'))[['Adv_ID', 'Wheelbase','Height','Width','Length']]\n",
    "for v in ['_all_views']:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        features_df = pd.read_csv(join(FEATURES,f'dvm_full_features_{split}_noOH{v}.csv'))\n",
    "        merged_df = features_df.merge(physical_df, on='Adv_ID')\n",
    "        physical_only_df = merged_df[['Wheelbase','Height','Width','Length','Bodytype']]\n",
    "\n",
    "        for attr in ['Wheelbase','Height','Width','Length']:\n",
    "            assert merged_df[attr].isna().sum()==0\n",
    "            assert (merged_df[attr]==0).sum()==0\n",
    "\n",
    "        # normalize physical attributes\n",
    "        for attr in ['Wheelbase','Height','Width','Length']:\n",
    "            merged_df[attr] = (merged_df[attr]-merged_df[attr].mean())/merged_df[attr].std()\n",
    "            physical_only_df[attr] = (physical_only_df[attr]-physical_only_df[attr].mean())/physical_only_df[attr].std()\n",
    "\n",
    "        # Drop unwanted cols\n",
    "        non_feature_columns = ['Adv_ID', 'Image_name', 'Genmodel_ID']\n",
    "        if v == '_all_views':\n",
    "            non_feature_columns.append('Predicted_viewpoint')\n",
    "        merged_df = merged_df.drop(non_feature_columns, axis=1)\n",
    "\n",
    "        merged_df_cols = merged_df.columns.tolist()\n",
    "        rearranged_cols = merged_df_cols[-4:]+merged_df_cols[:-4]\n",
    "        merged_df = merged_df[rearranged_cols]\n",
    "        check_or_save(merged_df, join(FEATURES,f'dvm_features_{split}_noOH{v}_physical_jittered_50.csv'), index=False, header=False)\n",
    "        check_or_save(physical_only_df, join(FEATURES,f'dvm_features_{split}_noOH{v}_physical_only_jittered_50.csv'), index=False, header=False)\n",
    "    lengths = torch.load(join(FEATURES,f'tabular_lengths{v}.pt'))\n",
    "    new_lengths = [1,1,1,1]\n",
    "    lengths = new_lengths + lengths\n",
    "    check_or_save(lengths, join(FEATURES,f'tabular_lengths{v}_physical.pt'))\n",
    "    lengths = [1,1,1,1,13]\n",
    "    check_or_save(lengths, join(FEATURES,f'tabular_lengths{v}_physical_only.pt'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Labels to Featues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in ['_all_views']:\n",
    "    for split in ['train', 'val']:\n",
    "        labels = torch.load(join(FEATURES,f'labels_model_all_{split}{v}.pt'))\n",
    "        features = pd.read_csv(join(FEATURES,f'dvm_features_{split}_noOH{v}_physical_jittered_50.csv'), header=None)\n",
    "        features['label'] = labels\n",
    "        check_or_save(features, join(FEATURES,f'dvm_features_{split}_noOH{v}_physical_jittered_50_labeled.csv'), index=False, header=False)\n",
    "    lengths = torch.load(join(FEATURES,f'tabular_lengths{v}_physical.pt'))\n",
    "    lengths.append(max(labels)+1)\n",
    "    check_or_save(lengths, join(FEATURES,f'tabular_lengths{v}_physical_labeled.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selfsuper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
